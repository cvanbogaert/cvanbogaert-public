{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T00:42:17.295213Z",
     "start_time": "2018-11-17T00:42:15.329771Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py:2744: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'dropout')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c11c39642aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m model = build_model(batch_size, num_classes, num_units,\n\u001b[0;32m--> 111\u001b[0;31m                     num_layers, dropout, num_timesteps, embedding_size)\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c11c39642aa0>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(batch_size, num_classes, num_units, num_layers, dropout, num_timesteps, embedding_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m         model.add(CuDNNLSTM(num_units, stateful=True,\n\u001b[1;32m     97\u001b[0m                             return_sequences=True, dropout=dropout))\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCuDNNLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     model.compile(loss='sparse_categorical_crossentropy',\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/cudnn_recurrent.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, return_sequences, return_state, go_backwards, stateful, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgo_backwards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/cudnn_recurrent.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, return_sequences, return_state, go_backwards, stateful, time_major, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# We invoke the base layer's initializer directly here because we do not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# want to create RNN cell instance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=bad-super-call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/checkpointable/base.pyc\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Mutable properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'dropout')"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Embedding, CuDNNLSTM, LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "import time\n",
    "import codecs\n",
    "import keras_preprocessing.text as kpt\n",
    "import json\n",
    "\n",
    "batch_size = 256\n",
    "num_units = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.5\n",
    "num_timesteps = 8\n",
    "embedding_size = 16\n",
    "\n",
    "# source text file information\n",
    "encoding = 'utf-8'\n",
    "fname = 'holmes_canon.txt'\n",
    "origin = 'https://sherlock-holm.es/stories/plain-text/cnus.txt'\n",
    "\n",
    "# how to tokenize the text\n",
    "char_level = True\n",
    "vocab_filename = 'vocab.json'\n",
    "\n",
    "# fetch the file\n",
    "filename = tf.keras.utils.get_file(fname, origin)\n",
    "with codecs.open(filename, encoding=encoding) as file:\n",
    "    text = file.read()\n",
    "\n",
    "# tokenize the text\n",
    "t = kpt.Tokenizer(char_level=char_level)\n",
    "t.fit_on_texts([text])\n",
    "tokens = np.array(t.texts_to_sequences([text])[0])\n",
    "\n",
    "vocab = t.word_index\n",
    "num_classes = len(vocab)+1\n",
    "\n",
    "with codecs.open(vocab_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(vocab, sort_keys=True, indent=4))\n",
    "\n",
    "\n",
    "class CategoricalSequenceFromTokens(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, tokens, batch_size, num_timesteps, num_classes):\n",
    "        self.tokens = tokens\n",
    "        self.batch_size = batch_size\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_tokens = len(self.tokens)\n",
    "#        self.num_batches = int(self.num_tokens/self.num_timesteps)\n",
    "        self.num_batches = self.num_tokens\n",
    "\n",
    "        self.base_times = np.random.randint(\n",
    "            self.num_tokens, high=None, size=self.batch_size)\n",
    "\n",
    "#        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #        ts = self.base_times + (index * self.num_timesteps)\n",
    "        ts = self.base_times + index\n",
    "\n",
    "        data_x = np.array([self.tokens.take(range(\n",
    "            tx, tx+self.num_timesteps), mode='wrap') for tx in ts])\n",
    "        data_y = np.array(\n",
    "            [self.tokens.take((tx+self.num_timesteps), mode='wrap') for tx in ts])\n",
    "\n",
    "        return data_x, data_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.base_times = np.random.randint(\n",
    "            self.num_tokens, high=None, size=self.batch_size)\n",
    "\n",
    "\n",
    "data_train = CategoricalSequenceFromTokens(\n",
    "    tokens, batch_size, num_timesteps, num_classes)\n",
    "data_val = CategoricalSequenceFromTokens(\n",
    "    tokens, batch_size, num_timesteps, num_classes)\n",
    "\n",
    "# model\n",
    "\n",
    "\n",
    "def build_model(batch_size, num_classes, num_units, num_layers, dropout, num_timesteps, embedding_size):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(Embedding(num_classes, embedding_size, batch_size=batch_size,\n",
    "                        input_length=num_timesteps))\n",
    "    if dropout > 0.0:\n",
    "        model.add(Dropout(dropout))\n",
    "    for i in range(num_layers-1):\n",
    "        model.add(CuDNNLSTM(num_units, stateful=True,\n",
    "                            return_sequences=True))\n",
    "    model.add(CuDNNLSTM(num_units, stateful=True))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "model = build_model(batch_size, num_classes, num_units,\n",
    "                    num_layers, dropout, num_timesteps, embedding_size)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "class my_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if np.random.random() < 2:\n",
    "            #            self.model.reset_states()\n",
    "            print\n",
    "#           print \"resetting states\"\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='logs' + '/' + str(time.time()),\n",
    "        histogram_freq=0,\n",
    "        batch_size=batch_size,\n",
    "        write_graph=True,\n",
    "        write_grads=True,\n",
    "        write_images=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best.hdf5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True, save_weights_only=False),\n",
    "    my_callback()\n",
    "]\n",
    "\n",
    "history = model.fit_generator(generator=data_train,\n",
    "                              validation_data=data_val,\n",
    "                              callbacks=callbacks,\n",
    "                              epochs=1000,\n",
    "                              steps_per_epoch=3000,\n",
    "                              validation_steps=1000,\n",
    "                              shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
