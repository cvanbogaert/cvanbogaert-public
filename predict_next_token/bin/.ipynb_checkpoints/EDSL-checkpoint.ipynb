{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T00:42:17.295213Z",
     "start_time": "2018-11-17T00:42:15.329771Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:432: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (256, 8, 16)              1152      \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)       (256, 512)                1085440   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (256, 72)                 36936     \n",
      "=================================================================\n",
      "Total params: 1,123,528\n",
      "Trainable params: 1,123,528\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2996/3000 [============================>.] - ETA: 0s - loss: 1.8816 - acc: 0.6016\n",
      "3000/3000 [==============================] - 55s 18ms/step - loss: 1.8811 - acc: 0.5352 - val_loss: 1.5209 - val_acc: 0.5462\n",
      "Epoch 2/1000\n",
      "1101/3000 [==========>...................] - ETA: 29s - loss: 1.4738 - acc: 0.5352"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Embedding, CuDNNLSTM, LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "import time\n",
    "import codecs\n",
    "import keras_preprocessing.text as kpt\n",
    "import json\n",
    "\n",
    "batch_size = 256\n",
    "num_units = 512\n",
    "num_layers = 1\n",
    "dropout = 0.0\n",
    "num_timesteps = 8\n",
    "embedding_size = 16\n",
    "\n",
    "# source text file information\n",
    "encoding = 'utf-8'\n",
    "fname = 'holmes_canon.txt'\n",
    "origin = 'https://sherlock-holm.es/stories/plain-text/cnus.txt'\n",
    "\n",
    "# how to tokenize the text\n",
    "char_level = True\n",
    "vocab_filename = 'vocab.json'\n",
    "\n",
    "# fetch the file\n",
    "filename = tf.keras.utils.get_file(fname, origin)\n",
    "with codecs.open(filename, encoding=encoding) as file:\n",
    "    text = file.read()\n",
    "\n",
    "# tokenize the text\n",
    "t = kpt.Tokenizer(char_level=char_level)\n",
    "t.fit_on_texts([text])\n",
    "tokens = np.array(t.texts_to_sequences([text])[0])\n",
    "\n",
    "vocab = t.word_index\n",
    "num_classes = len(vocab)+1\n",
    "\n",
    "with codecs.open(vocab_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(vocab, sort_keys=True, indent=4))\n",
    "\n",
    "\n",
    "class CategoricalSequenceFromTokens(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, tokens, batch_size, num_timesteps, num_classes):\n",
    "        self.tokens = tokens\n",
    "        self.batch_size = batch_size\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.num_tokens = len(self.tokens)\n",
    "#        self.num_batches = int(self.num_tokens/self.num_timesteps)\n",
    "        self.num_batches = self.num_tokens\n",
    "\n",
    "        self.base_times = np.random.randint(\n",
    "            self.num_tokens, high=None, size=self.batch_size)\n",
    "\n",
    "#        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #        ts = self.base_times + (index * self.num_timesteps)\n",
    "        ts = self.base_times + index\n",
    "\n",
    "        data_x = np.array([self.tokens.take(range(\n",
    "            tx, tx+self.num_timesteps), mode='wrap') for tx in ts])\n",
    "        data_y = np.array(\n",
    "            [self.tokens.take((tx+self.num_timesteps), mode='wrap') for tx in ts])\n",
    "\n",
    "        return data_x, data_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.base_times = np.random.randint(\n",
    "            self.num_tokens, high=None, size=self.batch_size)\n",
    "\n",
    "\n",
    "data_train = CategoricalSequenceFromTokens(\n",
    "    tokens, batch_size, num_timesteps, num_classes)\n",
    "data_val = CategoricalSequenceFromTokens(\n",
    "    tokens, batch_size, num_timesteps, num_classes)\n",
    "\n",
    "# model\n",
    "\n",
    "\n",
    "def build_model(batch_size, num_classes, num_units, num_layers, dropout, num_timesteps, embedding_size):\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(Embedding(num_classes, embedding_size, batch_size=batch_size,\n",
    "                        input_length=num_timesteps))\n",
    "\n",
    "    for i in range(num_layers-1):\n",
    "        model.add(CuDNNLSTM(num_units, stateful=True, return_sequences=True))\n",
    "        if dropout > 0.0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(CuDNNLSTM(num_units, stateful=True))\n",
    "    if dropout > 0.0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "model = build_model(batch_size, num_classes, num_units,\n",
    "                    num_layers, dropout, num_timesteps, embedding_size)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "class my_callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if np.random.random() < 2:\n",
    "            #            self.model.reset_states()\n",
    "            print\n",
    "#           print \"resetting states\"\n",
    "\n",
    "\n",
    "# training\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir='logs' + '/' + str(time.time()),\n",
    "        histogram_freq=0,\n",
    "        batch_size=batch_size,\n",
    "        write_graph=True,\n",
    "        write_grads=True,\n",
    "        write_images=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best.hdf5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True, save_weights_only=False),\n",
    "    my_callback()\n",
    "]\n",
    "\n",
    "history = model.fit_generator(generator=data_train,\n",
    "                              validation_data=data_val,\n",
    "                              callbacks=callbacks,\n",
    "                              epochs=1000,\n",
    "                              steps_per_epoch=3000,\n",
    "                              validation_steps=1000,\n",
    "                              shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
